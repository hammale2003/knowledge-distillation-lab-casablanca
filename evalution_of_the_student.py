import gradio as gr
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from transformers import AutoModel, AutoTokenizer, AutoImageProcessor, AutoModelForImageClassification
from datasets import load_dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import warnings
import time
import psutil

# Supprimer les avertissements sklearn pour une sortie plus propre
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")
import GPUtil
from PIL import Image
import io
import base64
from typing import Dict, List, Tuple
import copy
import random

def custom_collate_fn(batch):
    """Custom collate function to handle PIL images"""
    images = [item['image'] for item in batch]
    labels = [item['label'] for item in batch]
    return {'image': images, 'label': labels}

def ensure_rgb_images(images):
    """Convert all images to RGB format to ensure 3 dimensions"""
    rgb_images = []
    for img in images:
        if img.mode != 'RGB':
            img = img.convert('RGB')
        rgb_images.append(img)
    return rgb_images

# Configuration globale
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
NUM_CLASSES = 16  # RVL-CDIP a 16 classes

class ModelPerformanceTracker:
    def __init__(self):
        self.metrics = []
        
    def add_metrics(self, model_name, accuracy, precision, recall, f1, inference_time, memory_usage):
        self.metrics.append({
            'model': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'inference_time': inference_time,
            'memory_usage': memory_usage
        })
    
    def get_comparison_df(self):
        return pd.DataFrame(self.metrics)



# Chargement des mod√®les
def load_models():
    try:
        # Student model
        student_processor = AutoImageProcessor.from_pretrained("HAMMALE/vit-tiny-classifier-rvlcdip")
        student_model = AutoModelForImageClassification.from_pretrained("HAMMALE/vit-tiny-classifier-rvlcdip")
        
        # Teacher model
        teacher_processor = AutoImageProcessor.from_pretrained("microsoft/dit-large-finetuned-rvlcdip")
        teacher_model = AutoModelForImageClassification.from_pretrained("microsoft/dit-large-finetuned-rvlcdip")
        
        return student_model, student_processor, teacher_model, teacher_processor
    except Exception as e:
        return None, None, None, None

# Chargement du dataset
def load_rvl_dataset():
    try:
        ds = load_dataset("HAMMALE/rvl_cdip_OCR")
        return ds
    except Exception as e:
        gr.Warning(f"Erreur lors du chargement du dataset: {e}")
        return None

# Fonctions d'√©valuation
def evaluate_model(model, processor, dataloader, device):
    model.eval()
    model.to(device)
    
    predictions = []
    true_labels = []
    inference_times = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 100:  # Limiter pour les tests
                break
                
            images = batch['image']
            labels = batch['label']
            
            # Convert images to RGB to ensure 3 dimensions
            images = ensure_rgb_images(images)
            
            # Pr√©processing
            inputs = processor(images, return_tensors="pt").to(device)
            
            # Mesure du temps d'inf√©rence
            start_time = time.time()
            outputs = model(**inputs)
            inference_time = time.time() - start_time
            
            # Pr√©dictions
            preds = torch.argmax(outputs.logits, dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels)  # labels is already a list now
            inference_times.append(inference_time)
    
    # Calcul des m√©triques
    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')
    avg_inference_time = np.mean(inference_times)
    
    return accuracy, precision, recall, f1, avg_inference_time

def get_model_size(model):
    """Calcule la taille d'un mod√®le en MB"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    size_mb = (param_size + buffer_size) / 1024**2
    return size_mb

def get_memory_usage():
    """Obtient l'utilisation m√©moire actuelle"""
    return psutil.virtual_memory().percent

# Onglet 1: Comparaison des performances
def compare_models():
    try:
        # Chargement des mod√®les
        student_model, student_processor, teacher_model, teacher_processor = load_models()
        if student_model is None:
            return "Erreur lors du chargement des mod√®les", None, None
        
        # Chargement du dataset
        dataset = load_rvl_dataset()
        if dataset is None:
            return "Erreur lors du chargement du dataset", None, None
        
        # Pr√©paration des donn√©es de test
        test_data = dataset['test'].select(range(min(500, len(dataset['test']))))
        test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)
        
        tracker = ModelPerformanceTracker()
        
        # √âvaluation du student model
        print("√âvaluation du Student Model...")
        student_acc, student_prec, student_rec, student_f1, student_time = evaluate_model(
            student_model, student_processor, test_dataloader, DEVICE
        )
        student_size = get_model_size(student_model)
        memory_before = get_memory_usage()
        
        tracker.add_metrics(
            "Student (ViT-Tiny)", student_acc, student_prec, student_rec, student_f1, 
            student_time, student_size
        )
        
        # √âvaluation du teacher model
        print("√âvaluation du Teacher Model...")
        teacher_acc, teacher_prec, teacher_rec, teacher_f1, teacher_time = evaluate_model(
            teacher_model, teacher_processor, test_dataloader, DEVICE
        )
        teacher_size = get_model_size(teacher_model)
        
        tracker.add_metrics(
            "Teacher (DiT-Large)", teacher_acc, teacher_prec, teacher_rec, teacher_f1,
            teacher_time, teacher_size
        )
        
        # Cr√©ation du rapport de comparaison
        df = tracker.get_comparison_df()
        
        # Graphiques de comparaison
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # M√©triques de performance
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        models = df['model'].tolist()
        
        for i, metric in enumerate(metrics):
            ax = axes[i//2, i%2]
            values = df[metric].tolist()
            bars = ax.bar(models, values, color=['skyblue', 'lightcoral'])
            ax.set_title(f'{metric.capitalize()} Comparison')
            ax.set_ylabel(metric.capitalize())
            ax.set_ylim([0, 1])
            
            # Ajouter les valeurs sur les barres
            for bar, value in zip(bars, values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                       f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # Graphique de comparaison des temps d'inf√©rence et taille des mod√®les
        fig2, (ax2, ax3) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Temps d'inf√©rence
        bars1 = ax2.bar(models, df['inference_time'], color=['lightblue', 'lightcoral'])
        ax2.set_ylabel('Temps d\'inf√©rence (s)')
        ax2.set_title('Comparaison des Temps d\'Inf√©rence')
        ax2.grid(True, alpha=0.3)
        for bar, value in zip(bars1, df['inference_time']):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                    f'{value:.4f}s', ha='center', va='bottom')
        
        # Taille des mod√®les
        bars2 = ax3.bar(models, df['memory_usage'], color=['lightgreen', 'orange'])
        ax3.set_ylabel('Taille du mod√®le (MB)')
        ax3.set_title('Comparaison des Tailles des Mod√®les')
        ax3.grid(True, alpha=0.3)
        for bar, value in zip(bars2, df['memory_usage']):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{value:.1f} MB', ha='center', va='bottom')
        
        plt.tight_layout()
        
        report = f"""
        üìä RAPPORT DE COMPARAISON DES MOD√àLES
        
        üéØ PERFORMANCES:
        Student Model (ViT-Tiny):
        - Accuracy: {student_acc:.4f}
        - Precision: {student_prec:.4f}
        - Recall: {student_rec:.4f}
        - F1-Score: {student_f1:.4f}
        
        Teacher Model (DiT-Large):
        - Accuracy: {teacher_acc:.4f}
        - Precision: {teacher_prec:.4f}
        - Recall: {teacher_rec:.4f}
        - F1-Score: {teacher_f1:.4f}
        
        ‚ö° EFFICIENCE:
        Student Model:
        - Taille: {student_size:.2f} MB
        - Temps d'inf√©rence: {student_time:.4f}s
        
        Teacher Model:
        - Taille: {teacher_size:.2f} MB
        - Temps d'inf√©rence: {teacher_time:.4f}s
        
        üìà ANALYSE:
        - Ratio de compression: {teacher_size/student_size:.1f}x
        - Perte de performance: {(teacher_acc-student_acc)*100:.2f}%
        - Gain de vitesse: {teacher_time/student_time:.1f}x
        """
        
        return report, fig, fig2
        
    except Exception as e:
        return f"Erreur lors de la comparaison: {str(e)}", None, None

# Onglet 2: Apprentissage Continu




# √âvaluation R√âELLE de l'oubli catastrophique
class CatastrophicForgettingEvaluator:
    def __init__(self, model, processor):
        self.original_model = copy.deepcopy(model)
        self.model = model
        self.processor = processor
        self.baseline_performance = {}
        self.after_training_performance = {}
        self.continual_performance = {}
        self.fisher_information = {}
        self.optimal_params = {}
        self.teacher_logits = {}  # Pour LwF
        self.continual_method = None
        
    def evaluate_baseline(self, dataset):
        """√âvaluation baseline du mod√®le sur toutes les classes"""
        print("üìä √âvaluation baseline du mod√®le...")
        
        # √âvaluer sur un √©chantillon repr√©sentatif de chaque classe
        test_data = dataset['test'].select(range(min(1000, len(dataset['test']))))
        dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)
        
        # Performance globale
        accuracy, precision, recall, f1, _ = evaluate_model(self.model, self.processor, dataloader, DEVICE)
        
        # Performance par classe
        class_performance = self.evaluate_per_class(dataloader)
        
        self.baseline_performance = {
            'global': {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1},
            'per_class': class_performance
        }
        
        return self.baseline_performance
    
    def evaluate_per_class(self, dataloader):
        """√âvaluation d√©taill√©e par classe"""
        self.model.eval()
        self.model.to(DEVICE)
        
        class_predictions = {i: [] for i in range(NUM_CLASSES)}
        class_true_labels = {i: [] for i in range(NUM_CLASSES)}
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= 50:  # Limiter pour la vitesse
                    break
                    
                images = batch['image']
                labels = batch['label']
                images = ensure_rgb_images(images)
                
                inputs = self.processor(images, return_tensors="pt")
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                outputs = self.model(**inputs)
                
                preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()
                
                for pred, true_label in zip(preds, labels):
                    class_predictions[true_label].append(pred)
                    class_true_labels[true_label].append(true_label)
        
        # Calculer accuracy par classe
        class_accuracy = {}
        for class_id in range(NUM_CLASSES):
            if len(class_true_labels[class_id]) > 0:
                correct = sum(1 for p, t in zip(class_predictions[class_id], class_true_labels[class_id]) if p == t)
                class_accuracy[class_id] = correct / len(class_true_labels[class_id])
            else:
                class_accuracy[class_id] = 0.0
                
        return class_accuracy
    
    def fine_tune_on_subset(self, dataset, target_classes, epochs=3):
        """Fine-tune le mod√®le sur un sous-ensemble de classes (m√©thode standard)"""
        print(f"üéØ Fine-tuning STANDARD sur les classes: {target_classes}")
        
        # Filtrer les donn√©es pour les classes cibles
        def filter_classes(example):
            return example['label'] in target_classes
        
        filtered_data = dataset['train'].filter(filter_classes)
        train_subset = filtered_data.select(range(min(500, len(filtered_data))))
        
        # Configuration d'entra√Ænement
        self.model.train()
        self.model.to(DEVICE)
        optimizer = optim.AdamW(self.model.parameters(), lr=2e-5)
        criterion = nn.CrossEntropyLoss()
        
        dataloader = DataLoader(train_subset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)
        
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0
            
            for batch in dataloader:
                images = batch['image']
                labels = torch.tensor(batch['label'], dtype=torch.long).to(DEVICE)
                images = ensure_rgb_images(images)
                
                inputs = self.processor(images, return_tensors="pt")
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                
                outputs = self.model(**inputs)
                loss = criterion(outputs.logits, labels)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
            print(f"  √âpoque {epoch+1}/{epochs}, Loss moyenne: {epoch_loss/num_batches:.4f}")
    
    def evaluate_after_training(self, dataset):
        """√âvaluation apr√®s fine-tuning standard"""
        print("üìà √âvaluation apr√®s fine-tuning standard...")
        
        test_data = dataset['test'].select(range(min(1000, len(dataset['test']))))
        dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)
        
        # Performance globale
        accuracy, precision, recall, f1, _ = evaluate_model(self.model, self.processor, dataloader, DEVICE)
        
        # Performance par classe
        class_performance = self.evaluate_per_class(dataloader)
        
        self.after_training_performance = {
            'global': {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1},
            'per_class': class_performance
        }
        
        return self.after_training_performance
    
    def prepare_continual_learning(self, dataset, method='ewc', sample_size=200):
        """Pr√©parer les donn√©es n√©cessaires pour l'apprentissage continu"""
        print(f"üß† Pr√©paration pour l'apprentissage continu: {method.upper()}")
        
        self.continual_method = method
        
        if method == 'ewc':
            self.compute_fisher_information(dataset, sample_size)
        elif method == 'lwf':
            self.compute_teacher_logits(dataset, sample_size)
        elif method == 'mas':
            self.compute_mas_importance(dataset, sample_size)
        
    def compute_fisher_information(self, dataset, sample_size=200):
        """Calcule la Fisher Information Matrix pour EWC (version am√©lior√©e)"""
        print("üßÆ Calcul de la Fisher Information Matrix (EWC)...")
        
        # Prendre un √©chantillon √©quilibr√© pour le calcul
        sample_data = dataset['train'].select(range(min(sample_size, len(dataset['train']))))
        dataloader = DataLoader(sample_data, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)
        
        self.model.eval()
        self.model.to(DEVICE)
        
        # Sauvegarder les param√®tres optimaux
        self.optimal_params = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.optimal_params[name] = param.data.clone().detach()
        
        # Initialiser Fisher Information
        self.fisher_information = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.fisher_information[name] = torch.zeros_like(param.data).to(DEVICE)
        
        criterion = nn.CrossEntropyLoss()
        total_samples = 0
        
        # Calculer Fisher Information avec plusieurs passes
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 50:
                break
                
            images = batch['image']
            labels = torch.tensor(batch['label'], dtype=torch.long).to(DEVICE)
            images = ensure_rgb_images(images)
            
            inputs = self.processor(images, return_tensors="pt")
            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
            
            # Calculer gradients pour chaque √©chantillon
            for i in range(len(labels)):
                self.model.zero_grad()
                single_input = {k: v[i:i+1] for k, v in inputs.items()}
                single_label = labels[i:i+1]
                
                outputs = self.model(**single_input)
                loss = criterion(outputs.logits, single_label)
                loss.backward()
                
                # Accumuler Fisher Information (gradient^2)
                for name, param in self.model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        self.fisher_information[name] += param.grad.data ** 2
                
                total_samples += 1
        
        # Normaliser par le nombre d'√©chantillons
        for name in self.fisher_information:
            self.fisher_information[name] /= total_samples
            # Ajouter une petite constante pour √©viter les z√©ros
            self.fisher_information[name] += 1e-8
        
        print(f"‚úÖ Fisher Information calcul√©e sur {total_samples} √©chantillons")
        
    def compute_teacher_logits(self, dataset, sample_size=200):
        """Calcule les logits du mod√®le baseline pour LwF"""
        print("üéì Calcul des logits du mod√®le teacher (LwF)...")
        
        # Utiliser le mod√®le baseline comme teacher
        teacher_model = copy.deepcopy(self.original_model)
        teacher_model.eval()
        teacher_model.to(DEVICE)
        
        # Prendre un √©chantillon pour calculer les logits
        sample_data = dataset['train'].select(range(min(sample_size, len(dataset['train']))))
        dataloader = DataLoader(sample_data, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)
        
        self.teacher_logits = {}
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                images = batch['image']
                labels = batch['label']
                images = ensure_rgb_images(images)
                
                inputs = self.processor(images, return_tensors="pt")
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                
                outputs = teacher_model(**inputs)
                
                # Stocker les logits pour chaque √©chantillon
                for i, label in enumerate(labels):
                    sample_idx = batch_idx * 8 + i
                    self.teacher_logits[sample_idx] = outputs.logits[i].cpu().detach()
        
        print(f"‚úÖ Logits teacher calcul√©s pour {len(self.teacher_logits)} √©chantillons")
        
    def compute_mas_importance(self, dataset, sample_size=200):
        """Calcule l'importance des param√®tres pour MAS (Memory Aware Synapses)"""
        print("üß† Calcul de l'importance MAS...")
        
        # Prendre un √©chantillon
        sample_data = dataset['train'].select(range(min(sample_size, len(dataset['train']))))
        dataloader = DataLoader(sample_data, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)
        
        self.model.eval()
        self.model.to(DEVICE)
        
        # Sauvegarder les param√®tres optimaux
        self.optimal_params = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.optimal_params[name] = param.data.clone().detach()
        
        # Initialiser l'importance MAS
        self.mas_importance = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.mas_importance[name] = torch.zeros_like(param.data).to(DEVICE)
        
        total_samples = 0
        
        # Calculer l'importance bas√©e sur les gradients de l'output
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 50:
                break
                
            images = batch['image']
            labels = torch.tensor(batch['label'], dtype=torch.long).to(DEVICE)
            images = ensure_rgb_images(images)
            
            inputs = self.processor(images, return_tensors="pt")
            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
            
            # Calculer gradients pour chaque √©chantillon
            for i in range(len(labels)):
                self.model.zero_grad()
                single_input = {k: v[i:i+1] for k, v in inputs.items()}
                
                outputs = self.model(**single_input)
                # Utiliser la norme L2 de l'output au lieu de la loss
                l2_norm = torch.norm(outputs.logits, p=2)
                l2_norm.backward()
                
                # Accumuler l'importance (gradient absolu)
                for name, param in self.model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        self.mas_importance[name] += param.grad.data.abs()
                
                total_samples += 1
        
        # Normaliser
        for name in self.mas_importance:
            self.mas_importance[name] /= total_samples
            self.mas_importance[name] += 1e-8
        
        print(f"‚úÖ Importance MAS calcul√©e sur {total_samples} √©chantillons")
    
    def fine_tune_with_continual_learning(self, dataset, target_classes, epochs=3, 
                                        method='ewc', reg_lambda=1000):
        """Fine-tune avec techniques d'apprentissage continu"""
        
        # D√©finir la lambda selon la m√©thode
        if method == 'ewc':
            effective_lambda = reg_lambda
        elif method == 'lwf':
            effective_lambda = 0.5  # Lambda fixe pour LwF
        elif method == 'mas':
            effective_lambda = 1.0  # Lambda fixe pour MAS
        else:
            effective_lambda = 0.0
            
        print(f"üß† Fine-tuning avec {method.upper()} (Œª={effective_lambda}) sur les classes: {target_classes}")
        
        # Filtrer les donn√©es pour les classes cibles
        def filter_classes(example):
            return example['label'] in target_classes
        
        filtered_data = dataset['train'].filter(filter_classes)
        train_subset = filtered_data.select(range(min(500, len(filtered_data))))
        
        # Configuration d'entra√Ænement
        self.model.train()
        self.model.to(DEVICE)
        optimizer = optim.AdamW(self.model.parameters(), lr=2e-5)
        base_criterion = nn.CrossEntropyLoss()
        
        dataloader = DataLoader(train_subset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)
        
        for epoch in range(epochs):
            epoch_loss = 0
            epoch_reg_loss = 0
            num_batches = 0
            
            for batch_idx, batch in enumerate(dataloader):
                images = batch['image']
                labels = torch.tensor(batch['label'], dtype=torch.long).to(DEVICE)
                images = ensure_rgb_images(images)
                
                inputs = self.processor(images, return_tensors="pt")
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                
                outputs = self.model(**inputs)
                
                # Loss standard
                standard_loss = base_criterion(outputs.logits, labels)
                
                # Loss de r√©gularisation selon la m√©thode
                if method == 'ewc':
                    reg_loss = self._compute_ewc_loss()
                elif method == 'lwf':
                    reg_loss = self._compute_lwf_loss(outputs.logits, batch_idx)
                elif method == 'mas':
                    reg_loss = self._compute_mas_loss()
                else:
                    reg_loss = torch.tensor(0.0, device=DEVICE)
                
                # Loss totale
                total_loss = standard_loss + effective_lambda * reg_loss
                
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
                
                epoch_loss += standard_loss.item()
                epoch_reg_loss += reg_loss.item()
                num_batches += 1
            
            avg_standard_loss = epoch_loss / num_batches
            avg_reg_loss = epoch_reg_loss / num_batches
            print(f"  √âpoque {epoch+1}/{epochs}, Loss standard: {avg_standard_loss:.4f}, "
                  f"Loss {method.upper()}: {avg_reg_loss:.6f}")
    
    def _compute_ewc_loss(self):
        """Calcule la perte EWC"""
        ewc_loss = torch.tensor(0.0, device=DEVICE)
        
        for name, param in self.model.named_parameters():
            if name in self.fisher_information and name in self.optimal_params and param.requires_grad:
                fisher = self.fisher_information[name].to(DEVICE)
                optimal = self.optimal_params[name].to(DEVICE)
                ewc_loss += (fisher * (param - optimal) ** 2).sum()
        
        return ewc_loss
    
    def _compute_lwf_loss(self, student_logits, batch_idx):
        """Calcule la perte LwF (Learning without Forgetting)"""
        lwf_loss = torch.tensor(0.0, device=DEVICE)
        
        # Utiliser les logits du teacher stock√©s pour la distillation
        if hasattr(self, 'teacher_logits') and self.teacher_logits:
            temperature = 4.0
            kl_loss = nn.KLDivLoss(reduction='batchmean')
            
            # Utiliser les logits du teacher stock√©s
            batch_size = student_logits.size(0)
            start_idx = batch_idx * batch_size
            end_idx = start_idx + batch_size
            
            # V√©rifier si on a assez de teacher logits
            if end_idx <= len(self.teacher_logits):
                teacher_batch_logits = self.teacher_logits[start_idx:end_idx].to(DEVICE)
                
                # Distillation avec temp√©rature
                student_soft = F.log_softmax(student_logits / temperature, dim=1)
                teacher_soft = F.softmax(teacher_batch_logits / temperature, dim=1)
                
                lwf_loss = kl_loss(student_soft, teacher_soft) * (temperature ** 2)
        
        return lwf_loss
    
    def _compute_mas_loss(self):
        """Calcule la perte MAS"""
        mas_loss = torch.tensor(0.0, device=DEVICE)
        
        for name, param in self.model.named_parameters():
            if name in self.mas_importance and name in self.optimal_params and param.requires_grad:
                importance = self.mas_importance[name].to(DEVICE)
                optimal = self.optimal_params[name].to(DEVICE)
                mas_loss += (importance * (param - optimal) ** 2).sum()
        
        return mas_loss
    
    def evaluate_continual_performance(self, dataset):
        """√âvaluation apr√®s fine-tuning avec apprentissage continu"""
        print(f"üìà √âvaluation apr√®s fine-tuning avec {self.continual_method.upper()}...")
        
        test_data = dataset['test'].select(range(min(1000, len(dataset['test']))))
        dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)
        
        # Performance globale
        accuracy, precision, recall, f1, _ = evaluate_model(self.model, self.processor, dataloader, DEVICE)
        
        # Performance par classe
        class_performance = self.evaluate_per_class(dataloader)
        
        self.continual_performance = {
            'global': {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1},
            'per_class': class_performance
        }
        
        return self.continual_performance
    
    def calculate_forgetting(self, target_classes, include_continual=False):
        """Calcul de l'oubli catastrophique"""
        if not self.baseline_performance or not self.after_training_performance:
            return None
            
        # R√©sultats standards
        results = {
            'standard': self._calculate_forgetting_for_method(
                self.baseline_performance, self.after_training_performance, target_classes, 'Standard'
            )
        }
        
        # R√©sultats avec apprentissage continu si disponibles
        if include_continual and self.continual_performance:
            method_name = f"{self.continual_method.upper()}" if self.continual_method else "Continual"
            results['continual'] = self._calculate_forgetting_for_method(
                self.baseline_performance, self.continual_performance, target_classes, method_name
            )
        
        return results
    
    def _calculate_forgetting_for_method(self, baseline, after_training, target_classes, method_name):
        """Calcule l'oubli pour une m√©thode sp√©cifique"""
        # Oubli global
        global_forgetting = baseline['global']['accuracy'] - after_training['global']['accuracy']
        
        # Oubli par classe
        class_forgetting = {}
        preserved_classes = [i for i in range(NUM_CLASSES) if i not in target_classes]
        target_class_change = {}
        
        for class_id in range(NUM_CLASSES):
            baseline_acc = baseline['per_class'].get(class_id, 0)
            after_acc = after_training['per_class'].get(class_id, 0)
            forgetting = baseline_acc - after_acc
            
            if class_id in target_classes:
                target_class_change[class_id] = after_acc - baseline_acc  # Am√©lioration attendue
            else:
                class_forgetting[class_id] = forgetting  # Oubli des autres classes
        
        # Moyennes (avec gestion des cas vides)
        preserved_forgetting_values = [class_forgetting[i] for i in preserved_classes if i in class_forgetting and not np.isnan(class_forgetting[i])]
        target_improvement_values = [target_class_change[i] for i in target_classes if i in target_class_change and not np.isnan(target_class_change[i])]
        
        avg_forgetting_preserved = np.mean(preserved_forgetting_values) if preserved_forgetting_values else 0.0
        avg_improvement_target = np.mean(target_improvement_values) if target_improvement_values else 0.0
        
        return {
            'method': method_name,
            'global_forgetting': global_forgetting,
            'avg_forgetting_preserved_classes': avg_forgetting_preserved,
            'avg_improvement_target_classes': avg_improvement_target,
            'per_class_forgetting': class_forgetting,
            'target_class_changes': target_class_change,
            'preserved_classes': preserved_classes,
            'target_classes': target_classes
        }

def run_real_catastrophic_forgetting_evaluation(target_classes_str, fine_tuning_epochs, continual_method, reg_lambda):
    """√âvaluation de l'oubli catastrophique"""
    try:
        # Parse les classes cibles
        target_classes = [int(x.strip()) for x in target_classes_str.split(',') if x.strip().isdigit()]
        if not target_classes or any(c >= NUM_CLASSES for c in target_classes):
            return "‚ùå Erreur: Classes cibles invalides. Utilisez des nombres de 0 √† 15 s√©par√©s par des virgules.", None
        
        # Chargement des mod√®les
        student_model, student_processor, _, _ = load_models()
        if student_model is None:
            return "‚ùå Erreur lors du chargement du mod√®le student", None
        
        # Chargement du dataset
        dataset = load_rvl_dataset()
        if dataset is None:
            return "‚ùå Erreur lors du chargement du dataset", None
        
        # Cr√©ation de l'√©valuateur
        evaluator = CatastrophicForgettingEvaluator(student_model, student_processor)
        
        # 1. √âvaluation baseline
        baseline = evaluator.evaluate_baseline(dataset)
        
        # 2. Pr√©parer les mod√®les
        original_model = copy.deepcopy(evaluator.model)
        
        # 3. Fine-tuning standard
        print("\nüîß === FINE-TUNING STANDARD ===")
        evaluator.model = copy.deepcopy(original_model)
        evaluator.fine_tune_on_subset(dataset, target_classes, epochs=fine_tuning_epochs)
        after_training = evaluator.evaluate_after_training(dataset)
        
        # 4. Fine-tuning avec apprentissage continu si demand√©
        continual_results = None
        if continual_method != 'none':
            print(f"\nüß† === FINE-TUNING AVEC {continual_method.upper()} ===")
            # Restaurer le mod√®le original
            evaluator.model = copy.deepcopy(original_model)
            
            # Pr√©parer les donn√©es n√©cessaires pour l'apprentissage continu
            evaluator.prepare_continual_learning(dataset, method=continual_method)
            
            # Fine-tuning avec la m√©thode d'apprentissage continu
            evaluator.fine_tune_with_continual_learning(
                dataset, target_classes, epochs=fine_tuning_epochs, 
                method=continual_method, reg_lambda=reg_lambda
            )
            continual_results = evaluator.evaluate_continual_performance(dataset)
        
        # 5. Calcul de l'oubli catastrophique
        forgetting_analysis = evaluator.calculate_forgetting(target_classes, include_continual=(continual_method != 'none'))
        
        # Cr√©ation des graphiques
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        title = 'üß† Analyse de l\'Oubli Catastrophique'
        if continual_method != 'none':
            title += f' - Comparaison Standard vs {continual_method.upper()}'
        fig.suptitle(title, fontsize=16, fontweight='bold')
        
        # Pr√©paration des donn√©es
        standard_results = forgetting_analysis['standard']
        
        # Graphique 1: Comparaison performance globale
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        baseline_values = [baseline['global'][m] for m in metrics]
        after_values = [after_training['global'][m] for m in metrics]
        
        x = np.arange(len(metrics))
        
        if continual_method != 'none' and continual_results:
            # Comparaison √† 3 barres : Baseline, Standard, M√©thode Continue
            continual_values = [continual_results['global'][m] for m in metrics]
            width = 0.25
            
            bars1 = axes[0,0].bar(x - width, baseline_values, width, label='Baseline', color='skyblue', alpha=0.8)
            bars2 = axes[0,0].bar(x, after_values, width, label='Fine-tuning Standard', color='lightcoral', alpha=0.8)
            bars3 = axes[0,0].bar(x + width, continual_values, width, label=f'Fine-tuning {continual_method.upper()}', color='lightgreen', alpha=0.8)
            
            # Annotations pour 3 barres
            for i, (bar1, bar2, bar3, base_val, std_val, cont_val) in enumerate(zip(bars1, bars2, bars3, baseline_values, after_values, continual_values)):
                axes[0,0].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.01, 
                              f'{base_val:.3f}', ha='center', va='bottom', fontsize=8)
                axes[0,0].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.01, 
                              f'{std_val:.3f}', ha='center', va='bottom', fontsize=8)
                axes[0,0].text(bar3.get_x() + bar3.get_width()/2, bar3.get_height() + 0.01, 
                              f'{cont_val:.3f}', ha='center', va='bottom', fontsize=8)
        else:
            # Comparaison standard √† 2 barres
            width = 0.35
            bars1 = axes[0,0].bar(x - width/2, baseline_values, width, label='Baseline', color='skyblue', alpha=0.8)
            bars2 = axes[0,0].bar(x + width/2, after_values, width, label='Fine-tuning Standard', color='lightcoral', alpha=0.8)
            
            # Annotations pour 2 barres
            for i, (bar1, bar2, baseline_val, after_val) in enumerate(zip(bars1, bars2, baseline_values, after_values)):
                axes[0,0].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.01, 
                              f'{baseline_val:.3f}', ha='center', va='bottom', fontsize=9)
                axes[0,0].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.01, 
                              f'{after_val:.3f}', ha='center', va='bottom', fontsize=9)
        
        axes[0,0].set_xlabel('M√©triques')
        axes[0,0].set_ylabel('Score')
        axes[0,0].set_title('üìä Performance Globale: Comparaison des M√©thodes')
        axes[0,0].set_xticks(x)
        axes[0,0].set_xticklabels(metrics)
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        axes[0,0].set_ylim([0, 1])
        
        # Graphique 2: Comparaison de l'oubli catastrophique
        if continual_method != 'none' and 'continual' in forgetting_analysis:
            # Comparaison Standard vs M√©thode Continue
            methods = ['Standard', continual_method.upper()]
            global_forgetting_values = [
                forgetting_analysis['standard']['global_forgetting'],
                forgetting_analysis['continual']['global_forgetting']
            ]
            avg_forgetting_values = [
                forgetting_analysis['standard']['avg_forgetting_preserved_classes'],
                forgetting_analysis['continual']['avg_forgetting_preserved_classes']
            ]
            
            x = np.arange(len(methods))
            width = 0.35
            
            bars1 = axes[0,1].bar(x - width/2, global_forgetting_values, width, 
                                 label='Oubli Global', color=['orange', 'red'], alpha=0.8)
            bars2 = axes[0,1].bar(x + width/2, avg_forgetting_values, width, 
                                 label='Oubli Moyen (Classes Pr√©serv√©es)', color=['coral', 'darkred'], alpha=0.8)
            
            axes[0,1].set_xlabel('M√©thode')
            axes[0,1].set_ylabel('Score d\'Oubli')
            axes[0,1].set_title('üß† Comparaison de l\'Oubli Catastrophique')
            axes[0,1].set_xticks(x)
            axes[0,1].set_xticklabels(methods)
            axes[0,1].legend()
            axes[0,1].grid(True, alpha=0.3)
            axes[0,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            
            # Annotations
            for bar, value in zip(bars1, global_forgetting_values):
                axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005 if value >= 0 else bar.get_height() - 0.01, 
                              f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', fontsize=9)
            for bar, value in zip(bars2, avg_forgetting_values):
                axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005 if value >= 0 else bar.get_height() - 0.01, 
                              f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', fontsize=9)
                
        else:
            # Performance par classe (vue classique)
            preserved_classes = standard_results['preserved_classes']
            class_ids = list(range(NUM_CLASSES))
            baseline_class_acc = [baseline['per_class'].get(i, 0) for i in class_ids]
            after_class_acc = [after_training['per_class'].get(i, 0) for i in class_ids]
            
            # Couleurs diff√©rentes pour classes cibles vs pr√©serv√©es
            colors = ['red' if i in target_classes else 'blue' for i in class_ids]
            
            axes[0,1].bar(class_ids, baseline_class_acc, alpha=0.5, label='Baseline', color='gray')
            bars = axes[0,1].bar(class_ids, after_class_acc, alpha=0.8, label='Apr√®s Fine-tuning', color=colors)
            
            axes[0,1].set_xlabel('Classe')
            axes[0,1].set_ylabel('Accuracy')
            axes[0,1].set_title('üéØ Performance par Classe')
            axes[0,1].legend()
            axes[0,1].grid(True, alpha=0.3)
            axes[0,1].set_ylim([0, 1])
            
            # L√©gende pour les couleurs
            red_patch = mpatches.Patch(color='red', label='Classes cibles')
            blue_patch = mpatches.Patch(color='blue', label='Classes pr√©serv√©es')
            axes[0,1].legend(handles=[red_patch, blue_patch], loc='upper right')
        
        # Graphique 3: Changements de performance par classe (D√©gradation/Am√©lioration)
        preserved_classes = standard_results['preserved_classes']
        class_ids = list(range(NUM_CLASSES))
        baseline_class_acc = [baseline['per_class'].get(i, 0) for i in class_ids]
        after_class_acc = [after_training['per_class'].get(i, 0) for i in class_ids]
        
        if continual_method != 'none' and continual_results:
            # Comparaison des changements : Standard vs M√©thode Continue
            continual_class_acc = [continual_results['per_class'].get(i, 0) for i in class_ids]
            
            # Calcul des changements
            standard_changes = [after - baseline for after, baseline in zip(after_class_acc, baseline_class_acc)]
            continual_changes = [continual - baseline for continual, baseline in zip(continual_class_acc, baseline_class_acc)]
            
            x = np.arange(len(class_ids))
            width = 0.35
            
            # Couleurs bas√©es sur positif/n√©gatif
            standard_colors = ['green' if change >= 0 else 'red' for change in standard_changes]
            continual_colors = ['darkgreen' if change >= 0 else 'darkred' for change in continual_changes]
            
            bars1 = axes[1,0].bar(x - width/2, standard_changes, width, label='Changement Standard', 
                                 color=standard_colors, alpha=0.8)
            bars2 = axes[1,0].bar(x + width/2, continual_changes, width, label=f'Changement {continual_method.upper()}', 
                                 color=continual_colors, alpha=0.8)
            
            # Marquer les classes cibles avec des lignes verticales
            for i, class_id in enumerate(class_ids):
                if class_id in target_classes:
                    axes[1,0].axvline(x=i, color='blue', linestyle=':', alpha=0.6, linewidth=2)
            
            # Annotations pour les changements significatifs
            for i, (std_change, cont_change) in enumerate(zip(standard_changes, continual_changes)):
                if abs(std_change) > 0.05:  # Changement significatif
                    axes[1,0].text(i - width/2, std_change + (0.01 if std_change >= 0 else -0.02), 
                                  f'{std_change:+.2f}', ha='center', va='bottom' if std_change >= 0 else 'top', 
                                  fontsize=8, fontweight='bold')
                if abs(cont_change) > 0.05:  # Changement significatif
                    axes[1,0].text(i + width/2, cont_change + (0.01 if cont_change >= 0 else -0.02), 
                                  f'{cont_change:+.2f}', ha='center', va='bottom' if cont_change >= 0 else 'top', 
                                  fontsize=8, fontweight='bold')
            
        else:
            # Calculer les changements de performance (apr√®s - baseline)
            performance_changes = [after - baseline for after, baseline in zip(after_class_acc, baseline_class_acc)]
            
            # Couleurs bas√©es sur le type de classe et le changement
            colors = []
            for i, change in enumerate(performance_changes):
                if i in target_classes:
                    colors.append('green' if change >= 0 else 'orange')  # Classes cibles : vert si am√©lioration, orange si d√©gradation
                else:
                    colors.append('lightcoral' if change < 0 else 'lightblue')  # Classes pr√©serv√©es : rouge si oubli, bleu si stable/am√©lioration
            
            bars = axes[1,0].bar(class_ids, performance_changes, color=colors, alpha=0.8)
            
            # Annotations pour les changements significatifs
            for i, (bar, change) in enumerate(zip(bars, performance_changes)):
                if abs(change) > 0.05:  # Seulement pour les changements significatifs
                    axes[1,0].text(bar.get_x() + bar.get_width()/2, 
                                  bar.get_height() + (0.01 if change >= 0 else -0.02), 
                                  f'{change:+.2f}', ha='center', 
                                  va='bottom' if change >= 0 else 'top', 
                                  fontsize=9, fontweight='bold')
            
            # L√©gende personnalis√©e pour les couleurs
            target_improve_patch = mpatches.Patch(color='green', label='Classes cibles (am√©lior√©es)')
            target_degrade_patch = mpatches.Patch(color='orange', label='Classes cibles (d√©grad√©es)')
            preserved_forget_patch = mpatches.Patch(color='lightcoral', label='Classes pr√©serv√©es (oubli√©es)')
            preserved_stable_patch = mpatches.Patch(color='lightblue', label='Classes pr√©serv√©es (stables)')
            axes[1,0].legend(handles=[target_improve_patch, target_degrade_patch, preserved_forget_patch, preserved_stable_patch], 
                           loc='upper right', fontsize=8)
        
        axes[1,0].set_xlabel('Classe')
        axes[1,0].set_ylabel('Changement d\'Accuracy')
        axes[1,0].set_title('üìä Changements de Performance par Classe')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        axes[1,0].axhline(y=0, color='black', linestyle='--', alpha=0.5)  # Ligne de r√©f√©rence √† 0
        
        # Limites dynamiques bas√©es sur les donn√©es
        all_changes = performance_changes if not (continual_method != 'none' and continual_results) else standard_changes + continual_changes
        y_max = max(0.1, max(all_changes) * 1.2) if all_changes else 0.1
        y_min = min(-0.1, min(all_changes) * 1.2) if all_changes else -0.1
        axes[1,0].set_ylim([y_min, y_max])
        
        # Graphique 4: R√©sum√© des m√©triques cl√©s
        if continual_method != 'none' and 'continual' in forgetting_analysis:
            # Comparaison Standard vs M√©thode Continue sur m√©triques cl√©s
            metrics_names = ['Oubli Global', 'Oubli Classes\nPr√©serv√©es', 'Am√©lioration\nClasses Cibles']
            standard_metrics = [
                standard_results['global_forgetting'],
                standard_results['avg_forgetting_preserved_classes'],
                standard_results['avg_improvement_target_classes']
            ]
            continual_metrics = [
                forgetting_analysis['continual']['global_forgetting'],
                forgetting_analysis['continual']['avg_forgetting_preserved_classes'],
                forgetting_analysis['continual']['avg_improvement_target_classes']
            ]
            
            x = np.arange(len(metrics_names))
            width = 0.35
            
            bars1 = axes[1,1].bar(x - width/2, standard_metrics, width, label='Standard', 
                                 color=['red', 'orange', 'blue'], alpha=0.8)
            bars2 = axes[1,1].bar(x + width/2, continual_metrics, width, label=continual_method.upper(), 
                                 color=['darkred', 'darkorange', 'darkblue'], alpha=0.8)
            
            axes[1,1].set_xlabel('M√©triques')
            axes[1,1].set_ylabel('Score')
            axes[1,1].set_title(f'üìà Comparaison Standard vs {continual_method.upper()}')
            axes[1,1].set_xticks(x)
            axes[1,1].set_xticklabels(metrics_names)
            axes[1,1].legend()
            axes[1,1].grid(True, alpha=0.3)
            axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            
            # Annotations
            for bar, value in zip(bars1, standard_metrics):
                axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005 if value >= 0 else bar.get_height() - 0.01, 
                              f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)
            for bar, value in zip(bars2, continual_metrics):
                axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005 if value >= 0 else bar.get_height() - 0.01, 
                              f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', fontsize=8)
        else:
            # Vue standard sans EWC
            summary_labels = ['Oubli Global', 'Oubli Moyen\n(Classes Pr√©serv√©es)', 'Am√©lioration Moyenne\n(Classes Cibles)']
            summary_values = [
                standard_results['global_forgetting'],
                standard_results['avg_forgetting_preserved_classes'],
                standard_results['avg_improvement_target_classes']
            ]
            summary_colors = ['red' if v > 0 else 'green' for v in summary_values]
            
            bars4 = axes[1,1].bar(summary_labels, summary_values, color=summary_colors, alpha=0.8)
            axes[1,1].set_ylabel('Changement de Performance')
            axes[1,1].set_title('üìà R√©sum√©: M√©triques Cl√©s')
            axes[1,1].grid(True, alpha=0.3)
            axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            
            for bar, value in zip(bars4, summary_values):
                axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 if value >= 0 else bar.get_height() - 0.02, 
                              f'{value:.3f}', ha='center', va='bottom' if value >= 0 else 'top', fontweight='bold')
        
        plt.tight_layout()
        
        # Rapport d√©taill√©
        classes_names = [
            "letter", "form", "email", "handwritten", "advertisement", "scientific report",
            "scientific publication", "specification", "file folder", "news article", 
            "budget", "invoice", "presentation", "questionnaire", "resume", "memo"
        ]
        
        target_names = [classes_names[i] for i in target_classes]
        preserved_classes = standard_results['preserved_classes']
        
        report = f"""
üß† √âVALUATION R√âELLE DE L'OUBLI CATASTROPHIQUE
============================================

‚öôÔ∏è CONFIGURATION:
‚Ä¢ Mod√®le: Student (ViT-Tiny) pr√©-entra√Æn√©
‚Ä¢ Classes cibles pour fine-tuning: {target_classes} ({', '.join(target_names)})
‚Ä¢ Classes √† pr√©server: {len(preserved_classes)} autres classes
‚Ä¢ √âpoques de fine-tuning: {fine_tuning_epochs}
‚Ä¢ M√©thode d'apprentissage continu: {continual_method.upper() if continual_method != 'none' else 'Aucune'}
{f'‚Ä¢ Lambda r√©gularisation: {reg_lambda}' if continual_method != 'none' else ''}

üìä R√âSULTATS BASELINE (avant fine-tuning):
‚Ä¢ Accuracy globale: {baseline['global']['accuracy']:.4f}
‚Ä¢ Precision: {baseline['global']['precision']:.4f}
‚Ä¢ Recall: {baseline['global']['recall']:.4f}
‚Ä¢ F1-Score: {baseline['global']['f1']:.4f}

üéØ R√âSULTATS FINE-TUNING STANDARD:
‚Ä¢ Accuracy globale: {after_training['global']['accuracy']:.4f}
‚Ä¢ Precision: {after_training['global']['precision']:.4f}
‚Ä¢ Recall: {after_training['global']['recall']:.4f}
‚Ä¢ F1-Score: {after_training['global']['f1']:.4f}

üîª OUBLI CATASTROPHIQUE (STANDARD):
‚Ä¢ Oubli global: {standard_results['global_forgetting']:.4f} ({standard_results['global_forgetting']*100:+.2f}%)
‚Ä¢ Oubli moyen (classes pr√©serv√©es): {standard_results['avg_forgetting_preserved_classes']:.4f}
‚Ä¢ Am√©lioration moyenne (classes cibles): {standard_results['avg_improvement_target_classes']:.4f}
"""

        if continual_method != 'none' and continual_results:
            continual_results_data = forgetting_analysis['continual']
            report += f"""
üß† R√âSULTATS AVEC {continual_method.upper()}:
‚Ä¢ Accuracy globale: {continual_results['global']['accuracy']:.4f}
‚Ä¢ Precision: {continual_results['global']['precision']:.4f}
‚Ä¢ Recall: {continual_results['global']['recall']:.4f}
‚Ä¢ F1-Score: {continual_results['global']['f1']:.4f}

üîª OUBLI CATASTROPHIQUE ({continual_method.upper()}):
‚Ä¢ Oubli global: {continual_results_data['global_forgetting']:.4f} ({continual_results_data['global_forgetting']*100:+.2f}%)
‚Ä¢ Oubli moyen (classes pr√©serv√©es): {continual_results_data['avg_forgetting_preserved_classes']:.4f}
‚Ä¢ Am√©lioration moyenne (classes cibles): {continual_results_data['avg_improvement_target_classes']:.4f}

‚öñÔ∏è COMPARAISON {continual_method.upper()} vs STANDARD:
‚Ä¢ R√©duction d'oubli global: {(standard_results['global_forgetting'] - continual_results_data['global_forgetting'])*100:+.2f}%
‚Ä¢ R√©duction d'oubli (classes pr√©serv√©es): {(standard_results['avg_forgetting_preserved_classes'] - continual_results_data['avg_forgetting_preserved_classes'])*100:+.2f}%
‚Ä¢ Diff√©rence am√©lioration cibles: {(continual_results_data['avg_improvement_target_classes'] - standard_results['avg_improvement_target_classes'])*100:+.2f}%
"""

        report += "\nüèÜ INTERPR√âTATION:\n"
        
        # Analyse standard
        if standard_results['global_forgetting'] > 0.05:
            report += "‚ùå OUBLI SIGNIFICATIF (Standard)! Le mod√®le a oubli√© des connaissances importantes.\n"
        elif standard_results['global_forgetting'] > 0.02:
            report += "‚ö†Ô∏è Oubli mod√©r√© (Standard). Performance globale l√©g√®rement d√©grad√©e.\n"
        else:
            report += "‚úÖ Oubli minimal (Standard)! Le mod√®le a bien pr√©serv√© ses connaissances.\n"
        
        # Analyse de la m√©thode d'apprentissage continu si disponible
        if continual_method != 'none' and 'continual' in forgetting_analysis:
            continual_data = forgetting_analysis['continual']
            if continual_data['global_forgetting'] < standard_results['global_forgetting']:
                reduction = ((standard_results['global_forgetting'] - continual_data['global_forgetting']) / standard_results['global_forgetting']) * 100
                report += f"üéâ {continual_method.upper()} EFFICACE! R√©duction de l'oubli de {reduction:.1f}%\n"
            else:
                report += f"‚ö†Ô∏è {continual_method.upper()} n'a pas r√©duit l'oubli. Essayez d'ajuster lambda ou plus d'√©poques baseline.\n"
        
        if standard_results['avg_improvement_target_classes'] > 0.05:
            report += "üéØ Am√©lioration significative sur les classes cibles!\n"
        elif standard_results['avg_improvement_target_classes'] > 0:
            report += "üìà L√©g√®re am√©lioration sur les classes cibles.\n"
        else:
            report += "‚ö†Ô∏è Peu ou pas d'am√©lioration sur les classes cibles.\n"
        
        # Classes les plus affect√©es
        worst_forgetting = [(i, f) for i, f in standard_results['per_class_forgetting'].items() if f > 0.1]
        if worst_forgetting:
            worst_forgetting.sort(key=lambda x: x[1], reverse=True)
            report += f"\nüö® CLASSES LES PLUS AFFECT√âES (Standard):\n"
            for class_id, forgetting_score in worst_forgetting[:3]:
                report += f"‚Ä¢ {classes_names[class_id]} (classe {class_id}): {forgetting_score:.3f} d'oubli\n"
        
        report += f"""
üí° RECOMMANDATIONS:
"""
        if continual_method != 'none':
            if 'continual' in forgetting_analysis and forgetting_analysis['continual']['global_forgetting'] < standard_results['global_forgetting']:
                report += f"‚úÖ {continual_method.upper()} fonctionne bien! Continuez avec cette approche.\n"
            else:
                report += f"üîß Ajustez lambda {continual_method.upper()} (essayez des valeurs plus √©lev√©es: 5000, 10000).\n"
        else:
            report += "üß† Essayez une m√©thode d'apprentissage continu pour r√©duire l'oubli catastrophique.\n"
            
        report += """‚Ä¢ Si oubli > 0.05: Utiliser des techniques de regularisation (EWC, LwF)
‚Ä¢ Si am√©lioration faible: Augmenter les √©poques ou ajuster le learning rate
‚Ä¢ Pour r√©duire l'oubli: R√©duire le learning rate ou utiliser du rehearsal
‚Ä¢ EWC optimal: lambda entre 1000-10000 selon la complexit√© du mod√®le
"""
        
        return report, fig
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'√©valuation: {str(e)}", None

# Interface Gradio
def create_interface():
    with gr.Blocks(title="Comparaison Mod√®les & Apprentissage Continu", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ü§ñ Analyse Comparative: Student vs Teacher Models
        ## Interface d'√©valuation des performances et d'apprentissage continu
        """)
        
        with gr.Tabs():
            # Onglet 1: Comparaison des performances
            with gr.TabItem("üìä Comparaison des Performances"):
                gr.Markdown("""
                ### Comparaison Student Model vs Teacher Model
                Analyse des performances, efficience et trade-offs entre les mod√®les.
                """)
                
                compare_btn = gr.Button("üöÄ Lancer la Comparaison", variant="primary", size="lg")
                
                with gr.Row():
                    with gr.Column():
                        comparison_report = gr.Textbox(
                            label="üìã Rapport de Comparaison",
                            lines=20,
                            placeholder="Cliquez sur 'Lancer la Comparaison' pour voir les r√©sultats..."
                        )
                    
                    with gr.Column():
                        performance_plot = gr.Plot(label="üìà M√©triques de Performance")
                        efficiency_plot = gr.Plot(label="‚ö° Temps d'Inf√©rence et Tailles")
                
                compare_btn.click(
                    compare_models,
                    outputs=[comparison_report, performance_plot, efficiency_plot]
                )
            
            # Onglet 2: √âvaluation R√âELLE de l'Oubli Catastrophique
            with gr.TabItem("üß† Oubli Catastrophique"):
                gr.Markdown("""
                ### üî¨ √âvaluation de l'Oubli Catastrophique""")
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ‚öôÔ∏è Configuration de l'Exp√©rience")
                        
                        target_classes_input = gr.Textbox(
                            label="üéØ Classes cibles pour fine-tuning",
                            placeholder="0,1,2",
                            value="0,1,2",
                            info="Num√©ros des classes (0-15) s√©par√©s par des virgules. Ex: 0,1,2 pour letter,form,email"
                        )
                        
                        epochs_slider = gr.Slider(
                            minimum=1, maximum=10, value=3, step=1,
                            label="üìö √âpoques de fine-tuning",
                            info="Plus d'√©poques = plus d'oubli potentiel mais meilleure performance sur les cibles"
                        )
                        
                        continual_method_dropdown = gr.Dropdown(
                            choices=[
                                ("Aucune m√©thode (fine-tuning standard seulement)", "none"),
                                ("EWC - Elastic Weight Consolidation", "ewc"),
                                ("LwF - Learning without Forgetting", "lwf"),
                                ("MAS - Memory Aware Synapses", "mas")
                            ],
                            value="none",
                            label="üß† M√©thode d'apprentissage continu",
                            info="Choisir une technique pour r√©duire l'oubli catastrophique"
                        )
                        
                        reg_lambda_slider = gr.Slider(
                            minimum=100, maximum=10000, value=1000, step=100,
                            label="‚öñÔ∏è Lambda r√©gularisation (force de r√©gularisation)",
                            info="Plus √©lev√© = moins d'oubli mais peut limiter l'apprentissage des nouvelles t√¢ches",
                            visible=False
                        )
                        
                        real_forgetting_btn = gr.Button(
                            "üöÄ Lancer l'√âvaluation R√©elle", 
                            variant="primary", 
                            size="lg"
                        )
                        
                        # Afficher/masquer le slider lambda selon la m√©thode s√©lectionn√©e
                        def toggle_lambda_slider(continual_method):
                            return gr.update(visible=continual_method == "ewc")
                        
                        continual_method_dropdown.change(
                            toggle_lambda_slider,
                            inputs=[continual_method_dropdown],
                            outputs=[reg_lambda_slider]
                        )
                        
                        gr.Markdown("""
                        **üìö Classes RVL-CDIP (0-15) :**
                        - 0: letter, 1: form, 2: email, 3: handwritten
                        - 4: advertisement, 5: scientific report, 6: scientific publication
                        - 7: specification, 8: file folder, 9: news article
                        - 10: budget, 11: invoice, 12: presentation
                        - 13: questionnaire, 14: resume, 15: memo
                        
                        **üß† M√©thodes d'apprentissage continu :**
                        - **EWC** : Utilise la Fisher Information Matrix pour pr√©server les poids importants
                        - **LwF** : Distillation de connaissances avec le mod√®le original comme teacher
                        - **MAS** : Memory Aware Synapses bas√© sur l'importance des gradients
                        
                        **‚öñÔ∏è Lambda** : Contr√¥le le trade-off entre nouvelle t√¢che et pr√©servation (uniquement pour EWC)
                        """)
                    
                    with gr.Column(scale=2):
                        real_forgetting_plot = gr.Plot(
                            label="üìä Analyse de l'Oubli Catastrophique R√©el",
                            visible=True
                        )
                
                with gr.Row():
                    real_forgetting_report = gr.Textbox(
                        label="üìã Rapport d'Analyse D√©taill√©",
                        lines=20,
                        placeholder="Configurez l'exp√©rience et cliquez sur 'Lancer' pour mesurer l'oubli catastrophique r√©el...",
                        visible=True
                    )
                
                real_forgetting_btn.click(
                    run_real_catastrophic_forgetting_evaluation,
                    inputs=[target_classes_input, epochs_slider, continual_method_dropdown, reg_lambda_slider],
                    outputs=[real_forgetting_report, real_forgetting_plot]
                )
            
            # Onglet 3: Documentation
            with gr.TabItem("üìö Documentation"):
                gr.Markdown("""
                ### üìñ Guide d'utilisation
                
                #### üéØ Onglet 1: Comparaison des Performances
                - **Student Model**: HAMMALE/vit-tiny-classifier-rvlcdip (Vision Transformer compact)
                - **Teacher Model**: microsoft/dit-large-finetuned-rvlcdip (Document Image Transformer)
                - **Dataset**: HAMMALE/rvl_cdip_OCR (classification de documents)
                
                **M√©triques √©valu√©es:**
                - Accuracy, Precision, Recall, F1-Score
                - Temps d'inf√©rence et taille du mod√®le
                - Ratio de compression et perte de performance
                
                #### üß† Onglet 2: Oubli Catastrophique R√âEL
                **üéØ √âvaluation concr√®te** de l'oubli catastrophique du mod√®le Student :
                
                **Processus scientifique:**
                1. Mesure baseline du mod√®le pr√©-entra√Æn√© sur toutes les classes
                2. Fine-tuning r√©el sur classes sp√©cifiques s√©lectionn√©es
                3. Re-√©valuation et calcul pr√©cis de l'oubli par classe
                4. Analyse statistique et recommandations pratiques
                
                **M√©triques r√©elles:**
                - Oubli global et par classe (avant/apr√®s fine-tuning)
                - Performance diff√©rentielle sur classes cibles vs pr√©serv√©es
                - Identification des classes les plus vuln√©rables
                - Comparaison Standard vs m√©thodes d'apprentissage continu
                - Seuils d'alerte et recommandations techniques
                
                **üß† M√©thodes d'apprentissage continu disponibles:**
                - **EWC (Elastic Weight Consolidation)** : Utilise la Fisher Information Matrix pour pr√©server les poids importants
                - **LwF (Learning without Forgetting)** : Distillation de connaissances avec le mod√®le original comme teacher
                - **MAS (Memory Aware Synapses)** : Bas√© sur l'importance des gradients de sortie
                
                **‚öñÔ∏è Param√®tre Lambda** : Contr√¥le le trade-off entre nouvelle t√¢che et pr√©servation des anciennes (uniquement pour EWC)
                
                #### üîß Configuration Technique
                - Utilise PyTorch et Transformers
                - Support GPU/CPU automatique
                - Limitation des donn√©es pour optimiser la vitesse d'ex√©cution
                
                #### ‚ö†Ô∏è Notes Importantes
                
                **Performance :**
                - Comparaison des mod√®les : ~2-5 minutes
                - √âvaluation oubli catastrophique : ~5-15 minutes (selon √©poques et m√©thode)
                - Les r√©sultats peuvent varier selon le mat√©riel disponible
                
                **Donn√©es :**
                - Sous-ensembles utilis√©s pour optimiser la vitesse d'ex√©cution
                - R√©sultats repr√©sentatifs du comportement complet
                - Classes RVL-CDIP : 16 classes de documents (letters, forms, emails, etc.)
                
                **Recommandations d'utilisation :**
                - Commencez avec 2-3 classes cibles pour tester
                - Utilisez 3-5 √©poques pour un bon compromis temps/r√©sultats
                - Testez diff√©rentes m√©thodes continual learning pour comparer
                - Lambda entre 1000-5000 g√©n√©ralement optimal
                """)
    
    return demo

# Lancement de l'interface
if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=None,  # Trouve automatiquement un port libre
        show_error=True
    )
